import runpod
import torch
from diffusers import StableDiffusionXLPipeline, EulerDiscreteScheduler
import base64
from io import BytesIO
from PIL import Image
import os
import gc
import logging
import warnings
import json

# Configure logging and suppress specific warnings
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Suppress weight mismatch warnings that don't affect functionality
warnings.filterwarnings("ignore", category=UserWarning, message="Some weights of the model checkpoint*")
warnings.filterwarnings("ignore", category=UserWarning, message="Some weights of*were not initialized*")
warnings.filterwarnings("ignore", category=UserWarning, message="You should probably TRAIN this model*")

# --- Configuration ---
# The model path in the RunPod volume
MODEL_PATH = "/runpod-volume/photonicfusion-sdxl"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Global pipeline variable
pipeline = None

def check_and_fix_model_index():
    """æ£€æŸ¥å¹¶ä¿®å¤ model_index.json ä¸­çš„ None å€¼"""
    model_index_path = os.path.join(MODEL_PATH, "model_index.json")
    
    if not os.path.exists(model_index_path):
        logger.error(f"âŒ model_index.json ä¸å­˜åœ¨: {model_index_path}")
        return False
    
    try:
        with open(model_index_path, 'r') as f:
            model_index = json.load(f)
        
        logger.info("ğŸ” æ£€æŸ¥ model_index.json ç»„ä»¶æ˜ å°„...")
        
        # æ£€æŸ¥å¹¶ä¿®å¤ None å€¼
        fixed = False
        for key, value in model_index.items():
            if not key.startswith('_') and isinstance(value, list) and len(value) >= 2:
                component_type, component_name = value[0], value[1]
                
                if component_name is None or component_name == "null":
                    logger.warning(f"âš ï¸ å‘ç° None å€¼: {key} -> {component_name}")
                    
                    # å°è¯•è‡ªåŠ¨ä¿®å¤å¸¸è§ç»„ä»¶
                    if key == "feature_extractor":
                        model_index[key] = ["transformers", "CLIPImageProcessor"]
                        fixed = True
                        logger.info(f"ğŸ”§ ä¿®å¤: {key} -> CLIPImageProcessor")
                    elif key == "image_encoder":
                        model_index[key] = ["transformers", "CLIPVisionModelWithProjection"]
                        fixed = True
                        logger.info(f"ğŸ”§ ä¿®å¤: {key} -> CLIPVisionModelWithProjection")
                    elif key == "safety_checker":
                        # å®‰å…¨æ£€æŸ¥å™¨å¯ä»¥è®¾ä¸º null
                        model_index[key] = [None, None]
                        logger.info(f"ğŸ”§ è®¾ç½®: {key} -> null (å®‰å…¨)")
                    else:
                        logger.warning(f"âš ï¸ æ— æ³•è‡ªåŠ¨ä¿®å¤: {key}")
                else:
                    logger.info(f"âœ… {key}: {component_type} -> {component_name}")
        
        # å¦‚æœæœ‰ä¿®å¤ï¼Œä¿å­˜æ–‡ä»¶
        if fixed:
            backup_path = model_index_path + ".backup"
            os.rename(model_index_path, backup_path)
            logger.info(f"ğŸ’¾ å¤‡ä»½åŸæ–‡ä»¶: {backup_path}")
            
            with open(model_index_path, 'w') as f:
                json.dump(model_index, f, indent=2)
            logger.info(f"âœ… ä¿å­˜ä¿®å¤åçš„ model_index.json")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ å¤„ç† model_index.json å¤±è´¥: {e}")
        return False

def create_missing_configs():
    """åˆ›å»ºç¼ºå¤±çš„é…ç½®æ–‡ä»¶"""
    configs_to_create = {
        "tokenizer/tokenizer_config.json": {
            "add_prefix_space": False,
            "bos_token": {"__type": "AddedToken", "content": "<|startoftext|>", "lstrip": False, "normalized": True, "rstrip": False, "single_word": False},
            "clean_up_tokenization_spaces": True,
            "do_lower_case": True,
            "eos_token": {"__type": "AddedToken", "content": "<|endoftext|>", "lstrip": False, "normalized": True, "rstrip": False, "single_word": False},
            "errors": "replace",
            "model_max_length": 77,
            "name_or_path": "openai/clip-vit-large-patch14",
            "pad_token": "<|endoftext|>",
            "tokenizer_class": "CLIPTokenizer",
            "unk_token": {"__type": "AddedToken", "content": "<|endoftext|>", "lstrip": False, "normalized": True, "rstrip": False, "single_word": False}
        },
        
        "tokenizer_2/tokenizer_config.json": {
            "add_prefix_space": False,
            "bos_token": {"__type": "AddedToken", "content": "<|startoftext|>", "lstrip": False, "normalized": True, "rstrip": False, "single_word": False},
            "clean_up_tokenization_spaces": True,
            "do_lower_case": True,
            "eos_token": {"__type": "AddedToken", "content": "<|endoftext|>", "lstrip": False, "normalized": True, "rstrip": False, "single_word": False},
            "errors": "replace",
            "model_max_length": 77,
            "name_or_path": "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k",
            "pad_token": "<|endoftext|>",
            "tokenizer_class": "CLIPTokenizer",
            "unk_token": {"__type": "AddedToken", "content": "<|endoftext|>", "lstrip": False, "normalized": True, "rstrip": False, "single_word": False}
        },
        
        "scheduler/scheduler_config.json": {
            "_class_name": "EulerDiscreteScheduler",
            "_diffusers_version": "0.21.0",
            "beta_end": 0.012,
            "beta_schedule": "scaled_linear",
            "beta_start": 0.00085,
            "clip_sample": False,
            "num_train_timesteps": 1000,
            "prediction_type": "epsilon",
            "sample_max_value": 1.0,
            "set_alpha_to_one": False,
            "skip_prk_steps": True,
            "steps_offset": 1,
            "timestep_spacing": "leading",
            "trained_betas": None,
            "use_karras_sigmas": False
        }
    }
    
    # åˆ›å»ºtokenizerå¿…éœ€çš„ç‰¹æ®Šæ–‡ä»¶
    special_tokenizer_files = {
        "tokenizer/special_tokens_map.json": {
            "bos_token": "<|startoftext|>",
            "eos_token": "<|endoftext|>",
            "unk_token": "<|endoftext|>",
            "pad_token": "<|endoftext|>"
        },
        "tokenizer_2/special_tokens_map.json": {
            "bos_token": "<|startoftext|>",
            "eos_token": "<|endoftext|>",
            "unk_token": "<|endoftext|>",
            "pad_token": "<|endoftext|>"
        }
    }
    
    created_count = 0
    
    # åˆ›å»ºJSONé…ç½®æ–‡ä»¶
    for config_path, config_content in configs_to_create.items():
        full_path = os.path.join(MODEL_PATH, config_path)
        
        if not os.path.exists(full_path):
            try:
                os.makedirs(os.path.dirname(full_path), exist_ok=True)
                with open(full_path, 'w') as f:
                    json.dump(config_content, f, indent=2)
                logger.info(f"âœ… åˆ›å»ºé…ç½®æ–‡ä»¶: {config_path}")
                created_count += 1
            except Exception as e:
                logger.error(f"âŒ åˆ›å»º {config_path} å¤±è´¥: {e}")
        else:
            logger.info(f"â­ï¸ å·²å­˜åœ¨: {config_path}")
    
    # åˆ›å»ºtokenizerç‰¹æ®Šæ–‡ä»¶
    for file_path, content in special_tokenizer_files.items():
        full_path = os.path.join(MODEL_PATH, file_path)
        
        if not os.path.exists(full_path):
            try:
                os.makedirs(os.path.dirname(full_path), exist_ok=True)
                with open(full_path, 'w') as f:
                    json.dump(content, f, indent=2)
                logger.info(f"âœ… åˆ›å»ºç‰¹æ®Šæ–‡ä»¶: {file_path}")
                created_count += 1
            except Exception as e:
                logger.error(f"âŒ åˆ›å»º {file_path} å¤±è´¥: {e}")
        else:
            logger.info(f"â­ï¸ å·²å­˜åœ¨: {file_path}")
    
    # æ£€æŸ¥å¹¶åˆ›å»ºvocab.jsonå’Œmerges.txt (å¦‚æœç¼ºå¤±)
    tokenizer_dirs = ["tokenizer", "tokenizer_2"]
    for tokenizer_dir in tokenizer_dirs:
        tokenizer_path = os.path.join(MODEL_PATH, tokenizer_dir)
        
        # æ£€æŸ¥vocab.json
        vocab_path = os.path.join(tokenizer_path, "vocab.json")
        if not os.path.exists(vocab_path):
            logger.warning(f"âš ï¸ {tokenizer_dir}/vocab.json ç¼ºå¤±ï¼Œè¿™ä¼šå¯¼è‡´tokenizeråŠ è½½å¤±è´¥")
            logger.info(f"ğŸ“ å»ºè®®ä»Hugging Faceå®˜æ–¹CLIP tokenizerå¤åˆ¶vocab.jsonæ–‡ä»¶")
        
        # æ£€æŸ¥merges.txt
        merges_path = os.path.join(tokenizer_path, "merges.txt")
        if not os.path.exists(merges_path):
            logger.warning(f"âš ï¸ {tokenizer_dir}/merges.txt ç¼ºå¤±ï¼Œè¿™ä¼šå¯¼è‡´tokenizeråŠ è½½å¤±è´¥")
            logger.info(f"ğŸ“ å»ºè®®ä»Hugging Faceå®˜æ–¹CLIP tokenizerå¤åˆ¶merges.txtæ–‡ä»¶")
    
    return created_count

def diagnose_volume_structure():
    """è¯Šæ–­Volumeä¸­çš„æ¨¡å‹ç»“æ„"""
    logger.info(f"ğŸ” è¯Šæ–­æ¨¡å‹ç›®å½•ç»“æ„: {MODEL_PATH}")
    
    if not os.path.exists(MODEL_PATH):
        logger.error(f"âŒ Volumeè·¯å¾„ä¸å­˜åœ¨: {MODEL_PATH}")
        return False
    
    # æ£€æŸ¥å¿…éœ€ç»„ä»¶
    required_components = ["model_index.json", "unet", "vae", "text_encoder", "text_encoder_2"]
    missing_components = []
    
    for component in required_components:
        component_path = os.path.join(MODEL_PATH, component)
        if os.path.exists(component_path):
            logger.info(f"âœ… {component}")
        else:
            logger.error(f"âŒ ç¼ºå¤±: {component}")
            missing_components.append(component)
    
    if missing_components:
        logger.error(f"âŒ å…³é”®ç»„ä»¶ç¼ºå¤±: {missing_components}")
        return False
    
    # æ£€æŸ¥å¹¶ä¿®å¤ model_index.json
    if not check_and_fix_model_index():
        return False
    
    # åˆ›å»ºç¼ºå¤±çš„é…ç½®æ–‡ä»¶
    created = create_missing_configs()
    if created > 0:
        logger.info(f"âœ… åˆ›å»ºäº† {created} ä¸ªé…ç½®æ–‡ä»¶")
    
    return True

def fix_meta_tensors(model):
    """ä¿®å¤æ¨¡å‹ä¸­çš„ meta tensors"""
    import torch.nn as nn
    
    def _fix_module(module):
        for name, param in module.named_parameters(recurse=False):
            if param.is_meta:
                logger.warning(f"   ğŸ”§ Fixing meta tensor: {name}")
                # Create a new parameter with the same shape but on CPU
                new_param = nn.Parameter(
                    torch.empty_like(param, device='cpu', dtype=param.dtype)
                )
                # Initialize with small random values
                nn.init.normal_(new_param, mean=0.0, std=0.02)
                setattr(module, name, new_param)
        
        # Recursively fix child modules
        for child_module in module.children():
            _fix_module(child_module)
    
    _fix_module(model)
    return model

def load_model():
    """Load the PhotonicFusion SDXL model from RunPod volume"""
    global pipeline
    
    logger.info(f"Using device: {DEVICE}")
    logger.info(f"ğŸ“ Loading model from: {MODEL_PATH}")
    
    # é¦–å…ˆè¯Šæ–­å¹¶ä¿®å¤æ¨¡å‹ç»“æ„
    if not diagnose_volume_structure():
        raise RuntimeError(f"âŒ Volumeæ¨¡å‹ç»“æ„æ£€æŸ¥å¤±è´¥")
    
    # æ£€æŸ¥tokenizeræ–‡ä»¶ï¼Œå¦‚æœç¼ºå¤±å…³é”®æ–‡ä»¶ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤
    try:
        from transformers import CLIPTokenizer
        
        for tokenizer_dir in ["tokenizer", "tokenizer_2"]:
            tokenizer_path = os.path.join(MODEL_PATH, tokenizer_dir)
            vocab_path = os.path.join(tokenizer_path, "vocab.json")
            merges_path = os.path.join(tokenizer_path, "merges.txt")
            
            if not os.path.exists(vocab_path) or not os.path.exists(merges_path):
                logger.warning(f"âš ï¸ {tokenizer_dir} ç¼ºå¤±å…³é”®æ–‡ä»¶ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤...")
                
                try:
                    # ä¸‹è½½æ ‡å‡†çš„CLIP tokenizer
                    if tokenizer_dir == "tokenizer":
                        std_tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")
                    else:
                        std_tokenizer = CLIPTokenizer.from_pretrained("laion/CLIP-ViT-bigG-14-laion2B-39B-b160k")
                    
                    # ä¿å­˜åˆ°æœ¬åœ°
                    os.makedirs(tokenizer_path, exist_ok=True)
                    std_tokenizer.save_pretrained(tokenizer_path)
                    logger.info(f"âœ… å·²ä¸‹è½½å¹¶ä¿å­˜ {tokenizer_dir} æ–‡ä»¶")
                    
                except Exception as e:
                    logger.error(f"âŒ æ— æ³•è‡ªåŠ¨ä¿®å¤ {tokenizer_dir}: {e}")
                    # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œåˆ›å»ºæœ€å°åŒ–çš„tokenizeræ–‡ä»¶
                    logger.info(f"ğŸ”§ åˆ›å»ºæœ€å°åŒ–tokenizeræ–‡ä»¶...")
                    
                    if not os.path.exists(vocab_path):
                        # åˆ›å»ºæœ€å°çš„vocab.json
                        minimal_vocab = {}
                        for i in range(49408):
                            if i == 49406:
                                minimal_vocab["<|startoftext|>"] = i
                            elif i == 49407:
                                minimal_vocab["<|endoftext|>"] = i
                            else:
                                minimal_vocab[f"token_{i}"] = i
                        
                        with open(vocab_path, 'w', encoding='utf-8') as f:
                            json.dump(minimal_vocab, f)
                        logger.info(f"âœ… åˆ›å»ºæœ€å°åŒ– {tokenizer_dir}/vocab.json")
                    
                    if not os.path.exists(merges_path):
                        # åˆ›å»ºæœ€å°çš„merges.txt
                        minimal_merges = "\n".join([f"token_{i} token_{i+1}" for i in range(0, 1000, 2)])
                        
                        with open(merges_path, 'w', encoding='utf-8') as f:
                            f.write(minimal_merges)
                        logger.info(f"âœ… åˆ›å»ºæœ€å°åŒ– {tokenizer_dir}/merges.txt")
                        
    except Exception as e:
        logger.error(f"âŒ Tokenizeræ–‡ä»¶æ£€æŸ¥å¤±è´¥: {e}")
    
    try:
        # Load the pipeline with comprehensive error handling
        logger.info("ğŸ”„ Loading StableDiffusionXLPipeline...")
        
        # å°è¯•ä¸åŒçš„åŠ è½½ç­–ç•¥
        load_strategies = [
            # ç­–ç•¥1: ä½å†…å­˜æ¨¡å¼ + FP16
            {
                "torch_dtype": torch.float16 if DEVICE == "cuda" else torch.float32,
                "variant": "fp16" if DEVICE == "cuda" else None,
                "use_safetensors": True,
                "local_files_only": True,
                "safety_checker": None,
                "requires_safety_checker": False,
                "low_cpu_mem_usage": True,
                "device_map": "auto" if DEVICE == "cuda" else None
            },
            # ç­–ç•¥2: æ ‡å‡†FP16åŠ è½½ï¼ˆåŸç­–ç•¥1ï¼‰
            {
                "torch_dtype": torch.float16 if DEVICE == "cuda" else torch.float32,
                "variant": "fp16" if DEVICE == "cuda" else None,
                "use_safetensors": True,
                "local_files_only": True,
                "safety_checker": None,
                "requires_safety_checker": False
            },
            # ç­–ç•¥3: ä¸æŒ‡å®švariant
            {
                "torch_dtype": torch.float16 if DEVICE == "cuda" else torch.float32,
                "use_safetensors": True,
                "local_files_only": True,
                "safety_checker": None,
                "requires_safety_checker": False,
                "low_cpu_mem_usage": True
            },
            # ç­–ç•¥4: ä¸ä½¿ç”¨safetensors
            {
                "torch_dtype": torch.float16 if DEVICE == "cuda" else torch.float32,
                "local_files_only": True,
                "safety_checker": None,
                "requires_safety_checker": False
            },
            # ç­–ç•¥5: å…è®¸ç½‘ç»œä¸‹è½½ç¼ºå¤±ç»„ä»¶
            {
                "torch_dtype": torch.float16 if DEVICE == "cuda" else torch.float32,
                "use_safetensors": True,
                "safety_checker": None,
                "requires_safety_checker": False,
                "local_files_only": False,
                "low_cpu_mem_usage": True
            }
        ]
        
        last_error = None
        for i, strategy in enumerate(load_strategies, 1):
            logger.info(f"ğŸ”„ å°è¯•åŠ è½½ç­–ç•¥ {i}/{len(load_strategies)}...")
            
            # Suppress stderr temporarily to hide warnings
            import sys
            from io import StringIO
            
            old_stderr = sys.stderr
            sys.stderr = StringIO()
            
            try:
                pipeline = StableDiffusionXLPipeline.from_pretrained(
                    MODEL_PATH,
                    **strategy
                )
                
                # æ£€æŸ¥å¹¶ä¿®å¤ meta tensors
                logger.info(f"ğŸ” æ£€æŸ¥ meta tensors...")
                components_with_meta = []
                
                for component_name in ['vae', 'text_encoder', 'text_encoder_2', 'unet']:
                    component = getattr(pipeline, component_name, None)
                    if component is not None:
                        has_meta = any(param.is_meta for param in component.parameters())
                        if has_meta:
                            components_with_meta.append(component_name)
                
                if components_with_meta:
                    logger.warning(f"âš ï¸ å‘ç° meta tensors åœ¨: {components_with_meta}")
                    logger.info(f"ğŸ”§ ä¿®å¤ meta tensors...")
                    
                    for component_name in components_with_meta:
                        component = getattr(pipeline, component_name)
                        fixed_component = fix_meta_tensors(component)
                        setattr(pipeline, component_name, fixed_component)
                    
                    logger.info(f"âœ… Meta tensors å·²ä¿®å¤")
                else:
                    logger.info(f"âœ… æœªå‘ç° meta tensors")
                
                logger.info(f"âœ… ç­–ç•¥ {i} æˆåŠŸ!")
                break
                
            except Exception as e:
                last_error = e
                error_msg = str(e)
                
                # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                if "meta tensor" in error_msg.lower():
                    logger.warning(f"âš ï¸ ç­–ç•¥ {i} å¤±è´¥ (meta tensor): {error_msg[:150]}...")
                elif "safetensors" in error_msg.lower():
                    logger.warning(f"âš ï¸ ç­–ç•¥ {i} å¤±è´¥ (safetensors): {error_msg[:150]}...")
                elif "device" in error_msg.lower():
                    logger.warning(f"âš ï¸ ç­–ç•¥ {i} å¤±è´¥ (device): {error_msg[:150]}...")
                else:
                    logger.warning(f"âš ï¸ ç­–ç•¥ {i} å¤±è´¥: {error_msg[:150]}...")
                
                pipeline = None
                
            finally:
                # Restore stderr
                sys.stderr = old_stderr
        
        if pipeline is None:
            raise RuntimeError(f"æ‰€æœ‰åŠ è½½ç­–ç•¥éƒ½å¤±è´¥äº†ã€‚æœ€åé”™è¯¯: {last_error}")
        
        # Configure scheduler
        pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)
        
        # Move to device with meta tensor handling
        logger.info(f"ğŸ”„ Moving pipeline to {DEVICE}...")
        
        try:
            # First, try to initialize any meta tensors
            if hasattr(pipeline, '_apply_meta_tensor_fix'):
                pipeline._apply_meta_tensor_fix()
            
            # Move each component individually to handle meta tensors better
            components = ['vae', 'text_encoder', 'text_encoder_2', 'unet']
            for component_name in components:
                component = getattr(pipeline, component_name, None)
                if component is not None:
                    try:
                        logger.info(f"   ğŸ”„ Moving {component_name} to {DEVICE}...")
                        
                        # Check for meta tensors and handle them
                        has_meta = False
                        for param in component.parameters():
                            if param.is_meta:
                                has_meta = True
                                break
                        
                        if has_meta:
                            logger.warning(f"   âš ï¸ {component_name} has meta tensors, using to_empty()...")
                            # Use to_empty() for meta tensors
                            component = component.to_empty(device=DEVICE)
                            setattr(pipeline, component_name, component)
                        else:
                            # Normal move for non-meta tensors
                            component = component.to(DEVICE)
                            setattr(pipeline, component_name, component)
                        
                        logger.info(f"   âœ… {component_name} moved successfully")
                        
                    except Exception as e:
                        logger.warning(f"   âš ï¸ Failed to move {component_name}: {e}")
                        # If individual component fails, try to continue with others
                        continue
            
            logger.info("âœ… Pipeline components moved to device")
            
        except Exception as e:
            logger.error(f"âŒ Error moving pipeline to device: {e}")
            # Fallback: try to use the pipeline as-is
            logger.info("ğŸ”„ Attempting to continue with CPU/mixed precision...")
        
        if DEVICE == "cuda":
            try:
                pipeline.enable_attention_slicing()
                logger.info("âœ… Attention slicing enabled")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to enable attention slicing: {e}")
            
            try:
                pipeline.enable_model_cpu_offload()
                logger.info("âœ… Model CPU offload enabled")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to enable CPU offload: {e}")
            
            try:
                pipeline.enable_xformers_memory_efficient_attention()
                logger.info("âœ… XFormers enabled")
            except:
                logger.info("â„¹ï¸ XFormers not available")
        
        # Test the model
        logger.info("ğŸ§ª Testing model...")
        test_result = pipeline(
            prompt="test",
            num_inference_steps=1,
            width=64,
            height=64,
            output_type="pil"
        )
        
        logger.info("âœ… Model loaded and tested successfully!")
        return pipeline
        
    except Exception as e:
        logger.error(f"âŒ Failed to load model: {e}")
        
        # å¦‚æœæ˜¯NoneTypeé”™è¯¯ï¼Œæä¾›è¯¦ç»†è¯Šæ–­
        if "NoneType" in str(e):
            logger.error("ğŸ” æ£€æµ‹åˆ°NoneTypeé”™è¯¯ï¼Œè¿™é€šå¸¸æ˜¯ç”±äºtokenizeræ–‡ä»¶ç¼ºå¤±å¼•èµ·çš„")
            logger.error("è¯·æ£€æŸ¥tokenizerç›®å½•ä¸­çš„vocab.jsonå’Œmerges.txtæ–‡ä»¶")
        
        raise RuntimeError(f"Failed to load model from volume: {e}")

def generate_image(prompt, negative_prompt="", num_inference_steps=20, guidance_scale=7.0, 
                  width=1024, height=1024, seed=None):
    """Generate an image using the loaded pipeline"""
    global pipeline
    
    # Load model if not already loaded
    if pipeline is None:
        pipeline = load_model()
    
    logger.info(f"ğŸ¨ Generating image with prompt: {prompt[:50]}...")
    
    # Set seed for reproducibility
    if seed is not None:
        generator = torch.Generator(device=DEVICE).manual_seed(seed)
    else:
        generator = None
    
    try:
        # Generate image
        with torch.no_grad():
            result = pipeline(
                prompt=prompt,
                negative_prompt=negative_prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                width=width,
                height=height,
                generator=generator
            )
        
        # Convert to base64
        image = result.images[0]
        buffered = BytesIO()
        image.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode()
        
        # Cleanup
        gc.collect()
        if DEVICE == "cuda":
            torch.cuda.empty_cache()
        
        logger.info("âœ… Image generated successfully!")
        return img_str
        
    except Exception as e:
        logger.error(f"âŒ Image generation failed: {e}")
        raise

def handler(event):
    """RunPod handler function"""
    try:
        input_data = event['input']
        
        prompt = input_data.get('prompt', '')
        negative_prompt = input_data.get('negative_prompt', '')
        num_inference_steps = input_data.get('num_inference_steps', 20)
        guidance_scale = input_data.get('guidance_scale', 7.0)
        width = input_data.get('width', 1024)
        height = input_data.get('height', 1024)
        seed = input_data.get('seed', None)
        
        if not prompt:
            return {"error": "Prompt is required"}
        
        # Generate image
        image_base64 = generate_image(
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            width=width,
            height=height,
            seed=seed
        )
        
        return {
            "image": image_base64,
            "prompt": prompt,
            "negative_prompt": negative_prompt,
            "num_inference_steps": num_inference_steps,
            "guidance_scale": guidance_scale,
            "width": width,
            "height": height,
            "seed": seed
        }
        
    except Exception as e:
        logger.error(f"âŒ Handler error: {e}")
        return {"error": str(e)}

if __name__ == "__main__":
    logger.info("ğŸš€ Starting RunPod serverless worker...")
    
    # Pre-load the model for faster first request
    try:
        load_model()
        logger.info("âœ… Model pre-loaded successfully!")
    except Exception as e:
        logger.error(f"âŒ Model pre-load failed: {e}")
    
    # Start the RunPod worker
    runpod.serverless.start({"handler": handler}) 