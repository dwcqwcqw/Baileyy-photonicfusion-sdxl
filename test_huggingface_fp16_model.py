#!/usr/bin/env python3
"""
æµ‹è¯• HuggingFace ä¸Šçš„ PhotonicFusion SDXL fp16 variant æ¨¡å‹
"""

import torch
import time
from datetime import datetime

def test_fp16_variant():
    """æµ‹è¯• FP16 variant åŠ è½½"""
    print("ğŸ§ª æµ‹è¯• HuggingFace PhotonicFusion SDXL (FP16 Variant)")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒ
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {device}")
    
    if device == "cuda":
        gpu_name = torch.cuda.get_device_name(0)
        print(f"ğŸ® GPU: {gpu_name}")
        
        # æ˜¾ç¤ºæ˜¾å­˜ä¿¡æ¯
        memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
        memory_cached = torch.cuda.memory_reserved(0) / 1024**3
        print(f"ğŸ’¾ æ˜¾å­˜: {memory_allocated:.1f}GB å·²ç”¨ / {memory_cached:.1f}GB ç¼“å­˜")
    
    try:
        from diffusers import StableDiffusionXLPipeline
        print("âœ… diffusers å¯ç”¨")
    except ImportError:
        print("âŒ diffusers æœªå®‰è£…")
        return False
    
    repo_id = "Baileyy/photonicfusion-sdxl"
    
    # æµ‹è¯• 1: åŠ è½½ fp16 variant
    print(f"\nğŸ”„ æµ‹è¯• 1: åŠ è½½ fp16 variant from {repo_id}...")
    start_time = time.time()
    
    try:
        pipeline_fp16 = StableDiffusionXLPipeline.from_pretrained(
            repo_id,
            torch_dtype=torch.float16,
            variant="fp16",
            use_safetensors=True,
            device_map="auto" if device == "cuda" else None
        )
        
        load_time = time.time() - start_time
        print(f"âœ… FP16 variant åŠ è½½æˆåŠŸ ({load_time:.1f}s)")
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        if device == "cuda":
            pipeline_fp16 = pipeline_fp16.to(device)
        
        # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨
        if device == "cuda":
            memory_used = torch.cuda.memory_allocated(0) / 1024**3
            print(f"ğŸ’¾ æ¨¡å‹åŠ è½½åæ˜¾å­˜ä½¿ç”¨: {memory_used:.2f}GB")
        
    except Exception as e:
        print(f"âŒ FP16 variant åŠ è½½å¤±è´¥: {str(e)}")
        return False
    
    # æµ‹è¯• 2: ç”Ÿæˆå›¾åƒ
    print(f"\nğŸ”„ æµ‹è¯• 2: ç”Ÿæˆæµ‹è¯•å›¾åƒ...")
    
    prompt = "a beautiful sunset over mountains, photorealistic, high quality"
    negative_prompt = "blurry, low quality, distorted"
    
    start_time = time.time()
    
    try:
        with torch.no_grad():
            # ç”Ÿæˆå›¾åƒ
            result = pipeline_fp16(
                prompt=prompt,
                negative_prompt=negative_prompt,
                height=1024,
                width=1024,
                num_inference_steps=20,
                guidance_scale=7.0
            )
        
        generation_time = time.time() - start_time
        print(f"âœ… å›¾åƒç”ŸæˆæˆåŠŸ ({generation_time:.1f}s)")
        
        # ä¿å­˜å›¾åƒ
        image = result.images[0]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"photonicfusion_fp16_test_{timestamp}.png"
        image.save(filename)
        print(f"ğŸ’¾ å›¾åƒå·²ä¿å­˜: {filename}")
        
        # æ€§èƒ½ç»Ÿè®¡
        pixels_per_second = (1024 * 1024) / generation_time
        print(f"ğŸ“Š æ€§èƒ½: {pixels_per_second:,.0f} åƒç´ /ç§’")
        
        # æœ€ç»ˆæ˜¾å­˜ä½¿ç”¨
        if device == "cuda":
            final_memory = torch.cuda.memory_allocated(0) / 1024**3
            peak_memory = torch.cuda.max_memory_allocated(0) / 1024**3
            print(f"ğŸ’¾ å³°å€¼æ˜¾å­˜ä½¿ç”¨: {peak_memory:.2f}GB")
            print(f"ğŸ’¾ å½“å‰æ˜¾å­˜ä½¿ç”¨: {final_memory:.2f}GB")
        
    except Exception as e:
        print(f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥: {str(e)}")
        return False
    
    # æµ‹è¯• 3: æ¯”è¾ƒæ ‡å‡†åŠ è½½ vs fp16 variant
    print(f"\nğŸ”„ æµ‹è¯• 3: æ¯”è¾ƒæ ‡å‡†åŠ è½½...")
    
    try:
        start_time = time.time()
        pipeline_std = StableDiffusionXLPipeline.from_pretrained(
            repo_id,
            torch_dtype=torch.float16,
            use_safetensors=True,
            device_map="auto" if device == "cuda" else None
        )
        std_load_time = time.time() - start_time
        print(f"âœ… æ ‡å‡†ç‰ˆæœ¬åŠ è½½æˆåŠŸ ({std_load_time:.1f}s)")
        
        # æ¸…ç†å†…å­˜
        del pipeline_std
        if device == "cuda":
            torch.cuda.empty_cache()
        
        print(f"\nğŸ“Š åŠ è½½æ—¶é—´æ¯”è¾ƒ:")
        print(f"   FP16 variant: {load_time:.1f}s")
        print(f"   æ ‡å‡†ç‰ˆæœ¬: {std_load_time:.1f}s")
        print(f"   å·®å¼‚: {abs(load_time - std_load_time):.1f}s")
        
    except Exception as e:
        print(f"âš ï¸ æ ‡å‡†ç‰ˆæœ¬æµ‹è¯•å¤±è´¥: {str(e)}")
    
    # æ¸…ç†
    del pipeline_fp16
    if device == "cuda":
        torch.cuda.empty_cache()
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆ!")
    print(f"\nğŸ“‹ æ€»ç»“:")
    print(f"   âœ… FP16 variant å¯ç”¨")
    print(f"   âœ… åŠ è½½æ—¶é—´: {load_time:.1f}s")
    print(f"   âœ… ç”Ÿæˆæ—¶é—´: {generation_time:.1f}s")
    if device == "cuda":
        print(f"   âœ… å³°å€¼æ˜¾å­˜: {peak_memory:.2f}GB")
    print(f"   âœ… è¾“å‡ºæ–‡ä»¶: {filename}")
    
    return True

def test_model_info():
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    print("\nğŸ” è·å–æ¨¡å‹ä¿¡æ¯...")
    
    try:
        from huggingface_hub import HfApi
        
        api = HfApi()
        model_info = api.model_info("Baileyy/photonicfusion-sdxl")
        
        print(f"ğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
        print(f"   ä»“åº“: {model_info.modelId}")
        print(f"   æ ‡ç­¾: {model_info.tags}")
        print(f"   ä¸‹è½½é‡: {model_info.downloads}")
        print(f"   åˆ›å»ºæ—¶é—´: {model_info.created_at}")
        print(f"   æ›´æ–°æ—¶é—´: {model_info.last_modified}")
        
        # åˆ—å‡ºæ–‡ä»¶
        print(f"\nğŸ“ æ¨¡å‹æ–‡ä»¶:")
        for sibling in model_info.siblings:
            size_mb = sibling.size / (1024*1024) if sibling.size else 0
            file_type = "fp16" if "fp16" in sibling.rfilename else "standard"
            print(f"   ğŸ“„ {sibling.rfilename} ({size_mb:.1f}MB) [{file_type}]")
        
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è·å–æ¨¡å‹ä¿¡æ¯: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ PhotonicFusion SDXL FP16 Variant æµ‹è¯•å™¨")
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # è·å–æ¨¡å‹ä¿¡æ¯
    test_model_info()
    
    # æµ‹è¯• fp16 variant
    success = test_fp16_variant()
    
    print(f"\nâ° ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    if success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼FP16 variant å·¥ä½œæ­£å¸¸ï¼")
        print("\nğŸ’¡ æ¨èä½¿ç”¨æ–¹å¼:")
        print("pipeline = StableDiffusionXLPipeline.from_pretrained(")
        print("    'Baileyy/photonicfusion-sdxl',")
        print("    torch_dtype=torch.float16,")
        print("    variant='fp16',")
        print("    use_safetensors=True")
        print(")")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯")
    
    return success

if __name__ == "__main__":
    success = main()
    import sys
    sys.exit(0 if success else 1) 