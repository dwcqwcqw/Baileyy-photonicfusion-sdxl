#!/usr/bin/env python3
"""
æµ‹è¯• Meta Tensor ä¿®å¤
"""

import torch
import os
import logging
from diffusers import StableDiffusionXLPipeline

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def check_meta_tensors(model, model_name):
    """æ£€æŸ¥æ¨¡å‹ä¸­çš„ meta tensors"""
    meta_tensors = []
    total_params = 0
    
    for name, param in model.named_parameters():
        total_params += 1
        if param.is_meta:
            meta_tensors.append(name)
    
    logger.info(f"ğŸ“Š {model_name}: {total_params} å‚æ•°, {len(meta_tensors)} meta tensors")
    
    if meta_tensors:
        logger.warning(f"âš ï¸ Meta tensors åœ¨ {model_name}:")
        for name in meta_tensors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            logger.warning(f"   - {name}")
        if len(meta_tensors) > 5:
            logger.warning(f"   ... è¿˜æœ‰ {len(meta_tensors) - 5} ä¸ª")
    
    return len(meta_tensors) == 0

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    
    model_path = "/runpod-volume/photonicfusion-sdxl"
    
    if not os.path.exists(model_path):
        logger.error(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return False
    
    logger.info(f"ğŸ”„ æµ‹è¯•ä» {model_path} åŠ è½½æ¨¡å‹...")
    
    try:
        # ä½¿ç”¨ä½å†…å­˜æ¨¡å¼åŠ è½½
        pipeline = StableDiffusionXLPipeline.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            variant="fp16",
            use_safetensors=True,
            local_files_only=True,
            safety_checker=None,
            requires_safety_checker=False,
            low_cpu_mem_usage=True,
            device_map=None  # å…ˆä¸ç§»åŠ¨åˆ°GPU
        )
        
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        
        # æ£€æŸ¥å„ä¸ªç»„ä»¶çš„ meta tensors
        components_ok = True
        
        for component_name in ['vae', 'text_encoder', 'text_encoder_2', 'unet']:
            component = getattr(pipeline, component_name, None)
            if component is not None:
                is_ok = check_meta_tensors(component, component_name)
                components_ok = components_ok and is_ok
            else:
                logger.warning(f"âš ï¸ ç»„ä»¶ä¸å­˜åœ¨: {component_name}")
        
        if components_ok:
            logger.info("âœ… æ‰€æœ‰ç»„ä»¶éƒ½æ²¡æœ‰ meta tensors")
            
            # å°è¯•ç§»åŠ¨åˆ° GPU
            if torch.cuda.is_available():
                logger.info("ğŸ”„ å°è¯•ç§»åŠ¨åˆ° GPU...")
                try:
                    pipeline = pipeline.to("cuda")
                    logger.info("âœ… æˆåŠŸç§»åŠ¨åˆ° GPU")
                    
                    # ç®€å•æµ‹è¯•
                    logger.info("ğŸ§ª è¿è¡Œç®€å•æµ‹è¯•...")
                    result = pipeline(
                        prompt="a simple test",
                        num_inference_steps=1,
                        width=64,
                        height=64,
                        output_type="pil"
                    )
                    logger.info("âœ… æµ‹è¯•æˆåŠŸ!")
                    return True
                    
                except Exception as e:
                    logger.error(f"âŒ GPU ç§»åŠ¨å¤±è´¥: {e}")
                    return False
            else:
                logger.info("â„¹ï¸ CUDA ä¸å¯ç”¨ï¼Œè·³è¿‡ GPU æµ‹è¯•")
                return True
        else:
            logger.error("âŒ å‘ç° meta tensorsï¼Œéœ€è¦ä¿®å¤")
            return False
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    
    logger.info("ğŸš€ å¼€å§‹ Meta Tensor æ£€æŸ¥...")
    logger.info("=" * 80)
    
    success = test_model_loading()
    
    logger.info("=" * 80)
    if success:
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    else:
        logger.error("ğŸ’¥ æµ‹è¯•å¤±è´¥!")
    
    return success

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1) 